# 1. Let's Discuss the Problem - DB can perform less OPS - has low throughput.
## **The Problem: Real-Time Driver or Delivery Boy Tracking in Zomato/Uber**

In apps like **Uber** (for drivers) and **Zomato** (for delivery agents), one of the biggest technical challenges is **real-time tracking**. This means:

- Customers should see the **exact location** of their driver/delivery agent.
- The app should update **instantly** when the driver moves.
- The restaurant or driver should also get **live updates** about order pickup and drop-off.

If this tracking is **delayed, inaccurate, or slow**, it leads to:
âŒ A bad customer experience.  
âŒ Drivers missing important updates.  
âŒ Inefficient route optimization.  

## **Why is This Problem Difficult?**

### **1. High Volume of Location Updates** ðŸ“
- Every driver/delivery person sends **new GPS coordinates every few seconds**.
- If 100,000 drivers are online, thatâ€™s **100,000 location updates per second** (6 million updates per minute)!

### **2. Real-Time Processing is Required** âš¡
- Location data must be *processed* and *displayed* **instantly**.
- Traditional databases (like PostgreSQL or MySQL) are too slow for this.
- They can't handle so many operations(Read and Write) per second.

### **3. Multiple Services Need the Data** ðŸ”„
- **Customers** need to see the driverâ€™s live location.
- **Logistics team** needs real-time route optimization.
- **Restaurant** needs to know when the driver is arriving.
- **Notification system** needs to send alerts (e.g., "Your delivery agent is near").

A slow or inefficient system can cause major issues like:
- A driver has reached the destination, but the app still shows them far away.
- A delivery agent picks up the food, but the restaurant doesnâ€™t get an update.
- The customer doesnâ€™t get notifications, leading to confusion.

---

# 2. Kafka - The Solution

- Kafka has **âš¡high throughput** but **ðŸ”»temporary storage.**
- Query not possible in kafka.


# ðŸš€  Want to See Kafka in Action? - Node.js Kafka App for Real-Time Driver Location Tracking


## âœ… How is Data Inserted into the DB?

### 1ï¸âƒ£ Buffering with Batching (Recommended)
Instead of inserting data row by row, the consumer batches multiple Kafka messages and inserts them into the database at regular intervals (e.g., every 1 second or after collecting 1,000 records).  
This minimizes the number of expensive DB writes.

### ðŸ“Œ Implementation (Example using PostgreSQL & Kafka Consumer)

```javascript
const { Kafka } = require("kafkajs");
const { Pool } = require("pg");

const kafka = new Kafka({ clientId: "zomato-consumer", brokers: ["localhost:9092"] });
const consumer = kafka.consumer({ groupId: "location-consumers" });

const pool = new Pool({ connectionString: "postgres://user:password@localhost:5432/zomato" });

let buffer = []; // Stores messages temporarily
const BATCH_SIZE = 1000; // Adjust based on DB performance

const insertBatchToDB = async () => {
  if (buffer.length === 0) return;

  const values = buffer.map(l => `('${l.driver_id}', ${l.latitude}, ${l.longitude}, '${l.timestamp}')`).join(",");
  const query = `INSERT INTO driver_locations (driver_id, latitude, longitude, timestamp) VALUES ${values}`;
  
  try {
    await pool.query(query);
    console.log(`âœ… Inserted ${buffer.length} records into DB`);
    buffer = []; // Clear buffer after inserting
  } catch (error) {
    console.error("âŒ Error inserting into DB:", error);
  }
};

const runConsumer = async () => {
  await consumer.connect();
  await consumer.subscribe({ topic: "driver_location_updates", fromBeginning: false });

  await consumer.run({
    eachMessage: async ({ message }) => {
      const location = JSON.parse(message.value.toString());
      buffer.push(location);

      if (buffer.length >= BATCH_SIZE) {
        await insertBatchToDB();
      }
    },
  });

  setInterval(insertBatchToDB, 1000); // Inserts any remaining data every second
};

runConsumer();
```
